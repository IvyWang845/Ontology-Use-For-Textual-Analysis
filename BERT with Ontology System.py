# -*- coding: utf-8 -*-
"""Ontology_try.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uqnubkEcDLjsR3xKAr-chTGFZz82R2G7
"""

from transformers import BertTokenizer, BertForSequenceClassification, BertModel, BertConfig
from torch.utils.data import DataLoader
from transformers import AdamW
import pandas as pd
import torch
import random
import numpy as np
import torch.nn as nn
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from tqdm import tqdm
from spacy.scorer import PRFScore, ROCAUCScore, PRFScore
import logging
import nltk
import torch.nn.functional as F
from sklearn.metrics import cohen_kappa_score
syn_times = 5

"""read data"""
def read_data(file, classes):
    texts = []
    labels = []
    for idx, i in pd.read_csv(file).dropna(subset=classes +['Response']).iterrows():
        text = str(i['Response']).strip()
        multi_label = [0] * len(classes)
        for jdx, j in enumerate(classes):
            multi_label[jdx] = int(i[j])
        texts.append(text)
        labels.append(multi_label)
    assert len(texts) == len(labels)
    return texts, labels


"""create dataset"""
class CuDataset(torch.utils.data.Dataset):
    def __init__(self, encodings_list, labels):
        self.encodings_list = encodings_list
        self.labels = labels

    def __getitem__(self, idx):
        idx = int(idx)
        item_normal = {}
        input_list = []
        for i in range(len(self.encodings_list)):
            input = {key: torch.tensor(val[idx]) for key, val in self.encodings_list[i].items()}
            input_list.append(input)
        item_normal['input_list'] = input_list
        item_normal['labels'] = torch.tensor(self.labels[idx])        
        return item_normal

    def __len__(self):
        return len(self.labels)


"""model construction"""

class Model(BertForSequenceClassification):
    def __init__(self, config):
        super().__init__(config)
        self.dropout = nn.Dropout(0.3) #30% will be removed
        self.num_labels = 5 #5 levels

    def forward(self,
                input_ids=None,
                attention_mask=None,
                token_type_ids=None,
                position_ids=None,
                head_mask=None,
                inputs_embeds=None,
                labels=None,
                output_attentions=None,
                output_hidden_states=None,
                return_dict=None):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.bert(input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids,
                            position_ids=position_ids,
                            head_mask=head_mask,
                            inputs_embeds=inputs_embeds,
                            output_attentions=output_attentions,
                            output_hidden_states=output_hidden_states,
                            return_dict=return_dict)

        pooled_output = outputs[1]
        feat = self.dropout(pooled_output)

        return feat

class PredictorNet(nn.Module):
    def __init__(self, num_class=5, inc=768*5, temp=0.05):
        super(PredictorNet, self).__init__()
        self.fc = nn.Linear(inc, num_class, bias=False)                
        self.num_class = num_class
        self.temp = temp    

    def forward(self, x):         
        # forwarding        
        x = F.normalize(x)                    
        x = self.fc(x)
        return x/0.01
    
"""calculate metrics"""
@torch.no_grad()
def eval_model(model, model_classifier, eval_loader, threshold=0.5):
    model.eval()
    model_classifier.eval()
    labels_all = []
    preds_all = []
    
    data_iter = iter(eval_loader)
    print(len(eval_loader))
    for idx in range(len(eval_loader)):
        batch_dict = next(data_iter)

        feat_list = []
        for i in range(syn_times+1):
            input_ids = batch_dict['input_list'][i]['input_ids'].cuda()
            attention_mask = batch_dict['input_list'][i]['attention_mask'].cuda()
            token_type_ids = batch_dict['input_list'][i]['token_type_ids'].cuda()
            label = batch_dict['labels'].cuda()
            feat = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, position_ids=None, head_mask=None)
            feat_list.append(feat.unsqueeze(0))
        feat_list = torch.concat(feat_list, dim=0)            
        feat_list = torch.mean(feat_list,dim=0)
        logits = model_classifier(feat_list)

        # input_ids = batch['input_ids'].cuda()
        # attention_mask = batch['attention_mask'].cuda()
        # labels = batch['labels'].cuda()
        """loss calculation"""
        probs = torch.softmax(logits,dim=1)
        preds = torch.argmax(probs, dim=1)
        preds_all.extend(preds.detach().cpu().numpy())  # 拿到标签
        labels_all.extend(label.view(-1).cpu().numpy())
    preds_all = np.array(preds_all)
    labels_all = np.array(labels_all)
    accuracy_score = np.sum((preds_all==labels_all))/labels_all.shape[0]
    cohen_kappa = cohen_kappa_score(preds_all, labels_all)
    f_score = f1_score(preds_all, labels_all, average = "macro")
    
    #macro_f, accuracy = compute_metrics_split(labels, preds, threshold)
    #macro_f = accuracy_score(preds, labels)*100
    model.train()
    model_classifier.train()
    return accuracy_score, accuracy_score,cohen_kappa, f_score, preds_all, labels_all


def main():
    # SEED = 9999
    # random.seed(SEED)
    # np.random.seed(SEED)
    # torch.manual_seed(SEED)
    #torch.backends.cudnn.deterministic = True

    """load data"""
    student_raw_data = pd.read_csv('/home/lxh/Documents/Ontology/Biomass.csv', encoding='utf-8')
    student_raw_data.dropna()
    student_response_list = student_raw_data["Response"].tolist()

    classes = "Expertise".split('\t')
    classes = list(classes)
    texts, labels = read_data('/home/lxh/Documents/Ontology/all_expertise_data_augment.csv', classes)
    texts_val, labels_val = read_data('/home/lxh/Documents/Ontology/all_expertise_data.csv', classes)


    """training and testing set"""
    # train_texts, val_texts, train_labels, val_labels = train_test_split(
    #     texts, labels, test_size=0.3)
    train_texts = []
    train_labels = []
    for i in range(5):
        ids = 1293+i*1293
        train_texts += texts[ids:ids+1000]
        train_labels += labels[ids:ids+1000]
    val_texts = texts_val[1000:1293]
    val_labels = labels_val[1000:1293]
        
    
    # replace domain words
    file_vocab = open('./syn.txt', 'r')
    lines_list = file_vocab.readlines()
    dict_vocab = {}
    for i in range(len(lines_list)):
        words = lines_list[i][:-1].split(', ')
        dict_vocab[words[0]] = words[1:]    
    
    # train set
    train_texts_list = [[] for i in range(syn_times)]
    count_replaced_list = np.zeros(syn_times)
    for i in range(syn_times):
        for j in range(len(train_texts)):
            paragraph = train_texts[j]
            sentences = nltk.sent_tokenize(paragraph)
            sentences_syn = []
            for m in range(len(sentences)):
                sentence = sentences[m]
                words = nltk.word_tokenize(sentence)
                words = [w.lower() for w in words]
                words_syn = []
                for w in words:
                    if w in dict_vocab:
                        idx = np.min([i, len(dict_vocab[w])-1])
                        words_syn.append(dict_vocab[w][idx])
                        #words_syn.append(w)
                        count_replaced_list[idx] += 1
                    else:
                        words_syn.append(w)
                sentence_syn = ' '.join(words_syn)
                sentences_syn.append(sentence_syn)
            paragraph_syn = '. '.join(sentences_syn)
            train_texts_list[i].append(paragraph_syn)
    print(count_replaced_list)

    # val set
    val_texts_list = [[] for i in range(syn_times)]
    count_replaced_list = np.zeros(syn_times)
    for i in range(syn_times):
        for j in range(len(val_texts)):
            paragraph = val_texts[j]
            sentences = nltk.sent_tokenize(paragraph)
            sentences_syn = []
            for m in range(len(sentences)):
                sentence = sentences[m]
                words = nltk.word_tokenize(sentence)
                words = [w.lower() for w in words]
                words_syn = []
                for w in words:
                    if w in dict_vocab:
                        idx = np.min([i, len(dict_vocab[w])-1])
                        words_syn.append(dict_vocab[w][idx])
                        count_replaced_list[idx] += 1
                    else:
                        words_syn.append(w)
                sentence_syn = ' '.join(words_syn)
                sentences_syn.append(sentence_syn)
            paragraph_syn = '. '.join(sentences_syn)
            val_texts_list[i].append(paragraph_syn)
    print(count_replaced_list)
    
    train_texts_list.append(train_texts)
    val_texts_list.append(val_texts)
        
    """wordpiece"""
    # huggingface
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased") #bert-base-uncased  distilbert-base-uncased allenai/scibert_scivocab_cased  
    train_encodings_list = []
    val_encodings_list = []
    for i in range(len(train_texts_list)):
        train_encodings = tokenizer(train_texts_list[i], truncation=True, padding="max_length", max_length=128)
        train_encodings_list.append(train_encodings)
        
        val_encodings = tokenizer(val_texts_list[i], truncation=True, padding="max_length", max_length=128)
        val_encodings_list.append(val_encodings)
        

    train_dataset = CuDataset(train_encodings_list, train_labels)
    val_dataset = CuDataset(val_encodings_list, val_labels)

    """create dataloader"""
    batch_size = 10
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    eval_loader = DataLoader(val_dataset , batch_size=batch_size, shuffle=False)

    model = Model.from_pretrained('bert-base-uncased', num_labels=len(classes))
    model.cuda()
    model.train()

    #model_classifier = PredictorNet(num_class=5, inc=768*(syn_times+1), temp=0.05) #num_class = lables
    model_classifier = PredictorNet(num_class=5, inc=768, temp=0.05) #num_class = lables
    model_classifier.cuda()
    model_classifier.train()

    """learning rates"""
    optimizer_grouped_parameters = []
    for key, value in dict(model.named_parameters()).items():
        optimizer_grouped_parameters += [{'params': [value], 'lr': 1e-5,'weight_decay': 0.0005}]

    for key, value in dict(model_classifier.named_parameters()).items():
        optimizer_grouped_parameters += [{'params': [value], 'lr': 1e-4,'weight_decay': 0.0005}]
                                                
    optimizer = AdamW(optimizer_grouped_parameters)
    #optimizer = torch.optim.Adam(optimizer_grouped_parameters)

    """Start Training"""
    step = 0
    best_f1 = 0
    best_thre = None
    total_epoch = 20 #100

    for epoch in tqdm(range(total_epoch)):
        data_iter = iter(train_loader)

        for idx in range(len(data_iter)):
            batch_dict = next(data_iter)
            optimizer.zero_grad()
            
            # get feature
            feat_list = []
            for i in range(syn_times+1):
                input_ids = batch_dict['input_list'][i]['input_ids'].cuda()
                attention_mask = batch_dict['input_list'][i]['attention_mask'].cuda()
                token_type_ids = batch_dict['input_list'][i]['token_type_ids'].cuda()
                labels = batch_dict['labels'].cuda()
                feat = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, position_ids=None, head_mask=None)
                feat_list.append(feat.unsqueeze(0))
            #feat_list = torch.concat(feat_list, dim=1)            
            feat_list = torch.concat(feat_list, dim=0)       
            feat_list = torch.mean(feat_list, dim=0)     
            logits = model_classifier(feat_list)

            #loss_fct = torch.nn.BCEWithLogitsLoss()
            loss_fct = torch.nn.CrossEntropyLoss().cuda()
            loss = loss_fct(logits.view(-1, 5),labels.view(-1).type(torch.long))#num_label = 7 change
            if idx%20==0:
                print(loss)

            logging.info(
            f'Epoch-{epoch}, Step-{step}, Loss: {loss.cpu().detach().numpy()}')
            step += 1
            loss.backward()
            optimizer.step()
        logging.info('Eval:')
        for thr in range(1,2):
            #macro_f_train, accuracy_train, cohen_kappa, f_score, preds_all, labels_all = eval_model(model, model_classifier, train_loader, thr/10.)  # 评估模型
            macro_f_eval, accuracy_eval, cohen_kappa, f_score, preds_all, labels_all = eval_model(model, model_classifier, eval_loader, thr/10.)  # 评估模型
            if macro_f_eval > best_f1:
                # model.save_pretrained('model_best')  # 保存模型
                # tokenizer.save_pretrained('model_best')
                best_f1 = macro_f_eval
                best_thre = thr/10.
            logging.info(
            f'Epoch {epoch}, present best f1: {best_f1}, current f1: {macro_f_eval}, threshold={thr/10.}')
            print(best_f1,best_thre, accuracy_eval, cohen_kappa, f_score)
    logging.info(
            f'Train completed, best F1: {best_f1}, best threshold: {best_thre}')

    print(best_f1,best_thre)
    prediction = pd.DataFrame(preds_all, columns=['predictions'])
    actual = pd.DataFrame(labels_all, columns=['actual'])
    input = pd.DataFrame(val_texts, columns=['input'])
    pd_output = pd.concat([input,actual,prediction],axis = 1)
    pd_output.to_csv('onto_BERT_label_pred.csv')
    #output=pd.DataFrame(data={"id":X_test["id"],"Prediction":y_pred}) output.to_csv(path_or_buf="..\\output\\results.csv",index=False,quoting=3,sep=';')
    


if __name__ == '__main__':
    main()