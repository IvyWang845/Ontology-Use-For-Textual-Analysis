# Extending a Pretrained Language Model (BERT) using an Ontological Perspective to Classify Students’ Scientific Expertise Level from Written Responses
Official implementation of the proposed research paper "Extending a Pretrained Language Model (BERT) using an Ontological Perspective to Classify Students’ Scientific Expertise Level from Written Responses"
![image](https://github.com/IvyWang845/Ontology-Use-For-Textual-Analysis/blob/ba5b39f87f4f9bfd53b535fcecae5baa7e3ec1c6/Architecture.png)

Prerequisites:
---
1. Language: python
2. Ontology: EnvO http://environmentontology.org/ use its owl format: https://github.com/EnvironmentOntology/envo/blob/master/src/envo/envo-edit.owl
3. text2term package: https://pypi.org/project/text2term/
4. Huggingface Library for downloading BERT and SciBERT pretrained models and tokenization: https://huggingface.co/bert-base-uncase
